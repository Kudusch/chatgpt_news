---
title: "Diversity of News Sources in ChatGPT"
subtitle: "Note: Work in progess"
date-modified: 2025-07-07
bibliography: references.bib
format:  
  html:
    toc: true
    number-sections: true
    theme: default
    code-fold: true
    code-summary: "Show the code"
    css: main.css
---

```{r include = FALSE}
#| warning: false
#| echo: false

# Dependencies ----
library(rio)
library(dplyr)
library(tidyr)
library(widyr)
library(stringr)
library(lubridate)
library(jsonlite)
library(gt)
library(ggplot2)
library(scales)
library(ggrepel)

# Functions and colors ----
options(ggplot2.discrete.colour = c("#4e79a7", "#f28e2b", "#e15759", "#76b7b2", "#59a14f", "#edc948", "#b07aa1", "#ff9da7", "#9c755f", "#bab0ac"))
options(ggplot2.discrete.fill = c("#4e79a7", "#f28e2b", "#e15759", "#76b7b2", "#59a14f", "#edc948", "#b07aa1", "#ff9da7", "#9c755f", "#bab0ac"))

## Shannon Diversity Index: Higher values mean higher diversity
sdi <- function(counts) {
    props <- counts / sum(counts)
    -sum(props * log(props))
}
## Herfindahl–Hirschman index: Higher values mean higher concentration
hhi <- function(counts, limit=NA) {
    if (!is.na(limit)) {
        counts <- counts[1:limit]
    }
    props <- (counts/sum(counts, na.rm = TRUE))
    sum(props^2, na.rm = TRUE)
}
skew <- function(x) {
    sum(((x - mean(x))^3))/((length(x)-1)*(sd(x)^3))
}
get_condition <- function(uid) {
    factor(
        str_extract(uid, "[AB]"), 
        levels = c("A", "B"),
        labels = c("Regular", "Diverse")
    )
}

# Load data from json ----
links <- jsonlite::fromJSON("Data/links_api.json") |> 
    unnest(metadata) |> 
    unite(uid, -links, remove = FALSE) |> 
    unnest(links) |> 
    select(-Name) |> 
    rename(url = URL) |> 
    unite(date, year:minute, sep = "-") |> 
    mutate(date = strptime(date, format = "%Y-%m-%d-%H-%M")) |> 
    mutate(domain = urltools::url_parse(url)) |> 
    unnest(domain) |> 
    mutate(domain = str_remove(domain, "^www\\.")) |> 
    mutate(domain = str_remove(domain, "^www1\\.")) |> 
    mutate(url = str_remove(url, fixed("?utm_source=openai"))) |> 
    mutate(condition = get_condition(uid)) |> 
    distinct(uid, date, experiment, condition, url, domain)

links <- links |> 
    mutate(domain = ifelse(
        str_detect(domain, "news-pravda.com"),
        "news-pravda.com",
        domain
    ))

tranco <- import("data/top-1m.csv") |> 
    rename(tranco_rank = 1, domain = 2) |> 
    mutate(tranco_rank_inv = 1000001-tranco_rank)

manual_coding <- list.files("data", pattern = "coder_.*\\.xlsx", full.names = TRUE) |> 
    import_list(rbind = TRUE, setclass = "tibble") |> 
    mutate(coder = factor(Coder, levels = c("Tim", "Example", "Leonie", "Justin"))) |> 
    rename(category = Kategorie, is_springer = `Ist Springer?`) |> 
    mutate(domain = str_remove(Domain, "https://")) |> 
    mutate(is_springer = is_springer == "Ja") |> 
    mutate(is_springer = ifelse(is.na(is_springer), FALSE, is_springer)) |> 
    select(domain, category, is_springer, coder) |> 
    mutate(domain = ifelse(
        str_detect(domain, "news-pravda.com"),
        "news-pravda.com",
        domain
    )) |> 
    mutate(category = factor(
        category,
        c("private_newsmedia", "public_broadcaster", "local_media", "news_agency", "alternative_media", "encyclopedias", "organization", "other"),
        c("Private media", "Public broadcaster", "Local/regional media", "News agency", "Alternative media", "Encyclopedias", "Organization/Business", "Misc")
    ))

domains <- links |> 
    group_by(condition) |> 
    count(domain, sort = TRUE) |> 
    mutate(p = n/sum(n)) |> 
    mutate(cp = cumsum(p)) |> 
    ungroup() |> 
    left_join(tranco, by = "domain") |> 
    mutate(across(tranco_rank_inv, \(x) ifelse(is.na(x), 0, x))) |> 
    left_join(
        manual_coding |> group_by(domain) |> filter(!is.na(category)) |> arrange(coder) |> slice(1) |> filter(!is.na(category)), 
        by = "domain"
    ) |> 
    distinct()
```

## What sites are referenced by ChatGPT?

When prompted for current news from `{r} links$date |> min() |> format(format = "%d. %b %Y")` to `{r} links$date |> max() |> format(format = "%d. %b %Y")` in `{r} links$uid |> unique() |> length()` unique sessions, ChatGPT linked to a total of `{r} domains$domain |> unique() |> length()` unique domains. See @tbl-top-domains for an overview of linked domains, including the domains' share in the two experimental conditions (current news vs current news from diverse sources). Website popularity based on the Tranco website rankings [@Le_Pochat_2019]. `{r} 100*(domains$tranco_rank |> is.na() |> mean() |> round(digits = 2))` % of sites are not listed in the top 1 million sites ranked by tranco. A small number of linked domain names are not registered/invalid, including example domains (e.g. `news-site1.com`, `anderesbeispiel.de`), plausible sounding but fake sources (e.g. `technologieaktuell.de`, `aktionsseitedeutschland.de`) and typos/wrong top level domains (e.g. `euroma-mai.de` instead of `europa-mai.de`).

<!--https://web.archive.org/web/20250328100207/https://www.kapitalkompakt.de/impressum-->

```{r}
#| echo: false
#| warning: false
#| label: tbl-top-domains
#| tbl-cap: Top Domains

domains |> 
    select(-c(cp, category, is_springer, coder)) |> 
    group_by(domain, condition) |> 
    summarise(across(everything(), sum)) |> 
    mutate(n = sum(n)) |> 
    pivot_wider(names_from = condition, values_from = p, values_fill = 0) |> 
    arrange(-n) |> 
    ungroup() |>
    select(-tranco_rank_inv) |> 
    mutate(tranco_rank = ordinal(tranco_rank)) |> 
    mutate(domain = sprintf("https://%s", domain)) |> 
    gt() |> 
    opt_interactive(use_pagination = TRUE, page_size_default = 20) |> 
    fmt_percent(-c(domain, n, tranco_rank)) |> 
    fmt_url(domain, label = function(x) gsub("https://|www.", "", x)) |> 
    cols_align(align = "right", columns = tranco_rank) |> 
    cols_label(
        domain = "Domain",
        tranco_rank = "Tranco Rank"
    )
    

```

### Top sites over time

While some sites appear in the top 10 sources each week (e.g. `de.wikipedia.org`, `rnd.de`, and `reuters.com`), other sources appear only for a while in the top sources (e.g. `deutsche-handwerks-zeitung.de`, `bundesregierung.de`, `merkur.de`). See @tbl-top-domains-by-week-condition for a weekly top 10 of linked sources and @fig-domains-over-time for an overview of source prevalence over time for the top 25 sources.

```{r}
#| echo: false
#| warning: false
#| label: fig-domains-over-time
#| fig-cap: "Usage of Domains over Time"
#| fig-alt: ""
#| fig-width: 8
#| fig-height: 8
#| 
top_domains <- domains |> 
    group_by(domain) |> 
    summarise(n = sum(n)) |> 
    slice_max(n = 25, order_by = n) |> 
    pull(domain)

links |> 
    group_by(date = floor_date(date, unit = "1 day")) |> 
    count(domain, sort = TRUE) |> 
    filter(domain %in% top_domains) |> 
    mutate(domain = factor(domain, unique(domain))) |> 
    ggplot(aes(x = as.Date(date), y = n)) +
    geom_col(width = 1.1) +
    scale_x_date(date_labels = "KW %U", date_breaks = "2 weeks") +
    facet_wrap(vars(domain), nrow = 5, scales = "free_y") +
    labs(title = "Usage of Domains over Time", x = "Date", y = "Count")
    
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-top-domains-by-week-condition
#| tbl-cap: Top Domains by Week and Condition

links |> 
    group_by(date = format(floor_date(date, unit = "1 week"), "KW %U")) |> 
    count(domain, sort = TRUE) |> 
    slice_max(n = 10, order_by = n) |> 
    mutate(rank = 1:n()) |> 
    gt() |> 
    cols_move_to_start(rank) |> 
    cols_label(
        rank = "",
        domain = "Domain"
    )
```

### Domain count distribution

The distribution of domain counts (i.e. how often a site was referenced by ChatGPT) skews heavily right (`{r} domains |> group_by(domain) |> summarise(n = sum(n)) |> pull(n) |> skew() |> round(digits = 2)`), with a few sites referenced often and many sites being referenced only once.

@fig-domain-counts shows how the 198 of 816 most referenced sites account for 90% of all linked domains and 122 of 1183 sites in the diverse condition. Around 40% of all referenced sites are referenced only once and around 70% are referenced no more than 5 times (see @fig-domain-counts-cat).

```{r}
#| echo: false
#| warning: false
#| label: fig-domain-counts
#| fig-cap: "Distribution of Domain Counts"
#| fig-alt: ""
#| fig-width: 8
#| fig-height: 6

annotation_info <- domains |> 
    group_by(condition) |> 
    mutate(x = (1:n())) |> 
    filter(cp <= .9) |> 
    slice_max(order_by = cp) |> 
    ungroup() |> 
    select(condition, x) |> 
    pivot_wider(names_from = condition, values_from = x)

domains |> 
    group_by(condition) |> 
    mutate(x = (1:n())) |>
    ggplot(aes(x = x, y = cp, color = condition)) +
    geom_line(size = 1) +
    scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, .2)) +
    scale_x_continuous(labels = scales::number, breaks = \(lim) {seq(0, lim[2], 100)}) +
    labs(
        title = "Distribution of Domain Counts",
        subtitle = sprintf("The %s most used domains in the diverse condition and the %s most used domains in the regular condition\naccount for 90%% of all linked domains.", annotation_info$Regular, annotation_info$Diverse),
        y = "Share of linked Domains",
        x = "Domains",
        color = "Experimental condition"
    ) +
    geom_hline(yintercept = .9, color = "#ccc") +
    geom_vline(xintercept = annotation_info$Regular, color = "#4e79a7") +
    geom_vline(xintercept = annotation_info$Diverse, color = "#f28e2b") +
    theme(legend.position = "bottom")
```

```{r}
#| echo: false
#| warning: false
#| label: fig-domain-counts-cat
#| fig-cap: "Distribution of Domain Counts"
#| fig-alt: ""
#| fig-width: 8
#| fig-height: 5

domains |> 
    mutate(count_cat = case_when(
        n == 1 ~ "1",
        n <= 5 ~ "2-5",
        n <= 10 ~ "6-10",
        n <= 50 ~ "11-50",
        n <= 100 ~ "51-100",
        n > 100 ~ "> 100"
    )) |> 
    mutate(count_cat = factor(count_cat, levels = c("1","2-5","6-10","11-50","51-100","> 100"))) |> 
    group_by(condition) |> 
    count(count_cat, name = "n") |> 
    mutate(p = n/sum(n)) |> 
    ungroup() |> 
    ggplot(aes(x = count_cat, y = p, fill = condition)) +
    geom_col(position = "dodge") +
    scale_y_continuous(labels = percent) +
    labs(
        title = "Counts of Used Domain by Condition",
        subtitle = "Around 40% of all linked domains are used only once",
        y = "",
        x = "Count",
        fill = "Experimental condition"
    ) +
    theme(legend.position = "bottom")

```

### Domain categorization

```{r}
sample_meta <- domains |> 
    group_by(domain) |> 
    summarise(n = sum(n)) |> 
    ungroup() |> 
    group_by(in_sample = n >= 10) |> 
    summarise(n_refs = sum(n), unique_urls = n()) |> 
    mutate(p_refs = n_refs/sum(n_refs), p_urls = unique_urls/sum(unique_urls)) |> 
    filter(in_sample) |> 
    as.list()
```

A manual coding of the top domains (*N* = `{r} sample_meta$unique_urls`) representing `{r} sample_meta$p_refs |> round(digits = 2)` % of all references and `{r} sample_meta$p_urls |> round(digits = 2)` % of all linked sites reveals that ChatGPT links to a wide range of both journalistic and non-journalistic sources (see @tbl-domain-cats).

```{r}
#| echo: false
#| warning: false
#| label: tbl-domain-cats
#| tbl-cap: Categories of linked domains
domains |> 
    filter(!is.na(category)) |> 
    group_by(category, condition) |> 
    summarise(
        p = sum(p), 
        n = sum(n), 
        unique_domains = domain |> unique() |> length()
    ) |> 
    mutate(total_n = sum(n), unique_domains = sum(unique_domains)) |> 
    ungroup() |> 
    pivot_wider(names_from = condition, values_from = p, id_cols = c("category", "total_n", "unique_domains")) |> 
    gt() |> 
    fmt_percent(Regular:Diverse) |> 
    fmt_integer(total_n) |> 
    cols_label(
        category = "Category",
        total_n = "n",
        unique_domains = "Unique domains"
    )
```
Around one third of the manually coded sites are journalistic, including local/regional media, public broadcasters and news agencies. The remaining sources include online encyclopedias (mainly Wikipedia) and the websites of organizations and businesses, often as primary sources of a news story. ChatGPT is acting not as a news aggregator but assuming a journalistic role, providing primary sources.

ChatGPT also links to websites that scrape news sites and other sources and provide AI-generated 'news':

> Unsere Veröffentlichungspraxis basiert auf der Verbreitung neutral formulierter Nachrichtenartikel, die auf verschiedenen Quellen beruhen. Diese Quellen umfassen eine Vielzahl von Informationsquellen wie Nachrichtenmagazine, offene und geschlossene Foren, Social-Media-Plattformen (einschließlich Facebook, Twitter, LinkedIn, Reddit, Telegram, usw.), Darknet-Foren sowie E-Mail-Zuschriften mit relevanten Informationen. Um eine objektive Berichterstattung sicherzustellen, werden diese Informationen in Echtzeit abgeglichen und mittels einer umfangreichen Formel der Schwarmintelligenz sowie umfangreichen Autoritätskriterien analysiert. Dadurch werden die tatsächlichen Details ermittelt, Bias, Meinungen, Mutmaßungen und politische Richtungen neutralisiert und eine ausgewogene Berichterstattung gewährleistet. Als Aggregator verstehen wir uns ausschließlich als Presseschau zur öffentlichen Information bestehender Meldungen, um diese einfacher zugänglich zu machen.
>
> Die durch freie Redakteure und unserem KI-Unterstützungssystem analysierten Inhalte auf dieser Website unterliegen nur teilweise dem deutschen Urheberrecht, da sie von teilautomatisierten Prozessen generiert wurden.
>
> <https://die-nachrichten.at/pages/kodex>, 2025-07-07

<!--https://web.archive.org/web/20250519023336/https://www.vorreiter-zeitung.de/post/30-jahre-giftnotruf-erfurt-->

ChatGPT also references alternative news outlets [@Holt09082019], like `tichyseinblick.de` and `telepolis.de` and media outlets connected to the Russian government like `deutschetageszeitung.de` and `news-pravda.com`.

Outlets published by Axel Springer publishing do not seem to be preferred by ChatGPT over other outlets (see @fig-springer-domains).

```{r}
#| echo: false
#| warning: false
#| label: fig-springer-domains
#| fig-cap: "Domains related with Axel Springer publishing"
#| fig-alt: "Column chart showing how Springer publications don't appear in the top 15"

domains |> 
    filter(!is.na(is_springer)) |> 
    group_by(condition) |> 
    arrange(-n) |> 
    mutate(x = 1:n()) |> 
    ungroup() |> 
    mutate(label = ifelse(is_springer, domain, NA)) |> 
    mutate(is_springer = ifelse(is_springer, "Axel Springer", "Other publisher")) |> 
    ggplot(aes(x = x, y = n, fill = is_springer)) +
    geom_col(width = 1.1) +
    geom_label_repel(aes(label = label), nudge_y = 800, fill = options()$ggplot2.discrete.colour[6], min.segment.length = 0) +
    scale_fill_manual(values = options()$ggplot2.discrete.colour[c(6,5)]) +
    facet_wrap(vars(condition), nrow = 2, scales = "free") +
    labs(title = "",x = "", y = "Count", fill = "") +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), legend.position = "bottom")
```


## What are the differences between the experimental conditions?

### Domains

Some sites are referenced roughly equally often in both conditions (e.g. `deutschlandfunk.de`, `spiegel.de`, `faz.net`)[^1], some are referenced more in the diverse condition (e.g. `news-pravda.com`[^2], `taz.de`, `swr.de`) and others in the regular condition (e.g. `lemonde.fr`, `ft.com`, `apnews.com`). Most sites (\~ 99%) appear either (within a 5% deviation) in the regular or in the diverse condition (see @fig-domain-conditions).

[^1]: equal use within \~5% deviation

[^2]: a [russian propaganda network](https://en.wikipedia.org/wiki/Pravda_network)

```{r}
#| echo: false
#| warning: false
#| label: fig-domain-conditions
#| fig-cap: "Usage of Domains in the Experimental Conditions"
#| fig-alt: ""
#| fig-width: 8
#| fig-height: 8

domains |> 
    group_by(domain) |> 
    mutate(total_n = sum(n)) |> 
    ungroup() |> 
    pivot_wider(
        names_from = condition, 
        values_from = p, 
        id_cols = c(domain, total_n), 
        values_fill = 0
    ) |> 
    mutate(d = Diverse / Regular) |> 
    mutate(d = log(d)) |> 
    mutate(condition = case_when(
        d < -0.05 ~ "Regular",
        d > 0.05 ~ "Diverse",
        between(d, -.05, .05) ~ "Both"
    )) |>
    mutate(label = ifelse(between(d, -.75, .75), NA, domain)) |> 
    mutate(label = ifelse(total_n < 100, NA, label)) |> 
    ggplot(aes(x = Diverse, y = Regular)) +
    geom_abline(slope = 1, intercept = 0) +
    geom_point(aes(size = total_n, color = condition)) +
    geom_label_repel(aes(label = label, fill = condition), min.segment.length = 0) +
    scale_color_manual(values = c("Regular"="#4e79a7", "Diverse"="#f28e2b", "Both"="gray")) +
    scale_fill_manual(values = c("Regular"="#4e79a7", "Diverse"="#f28e2b", "Both"="gray")) +
    scale_y_log10(labels = scales::percent) +
    scale_x_log10(labels = scales::percent) +
    theme(legend.position = "bottom")
```

@tbl-top-domains-by-condition shows the domains that are strongly associated with either the diverse or the regular condition, and those that are represented equally often in both conditions.

```{r}
#| echo: false
#| warning: false
#| label: tbl-top-domains-by-condition
#| tbl-cap: Top Domains by Condition
domains |> 
    group_by(domain) |> 
    mutate(total_n = sum(n)) |> 
    ungroup() |> 
    pivot_wider(
        names_from = condition, 
        values_from = p, 
        id_cols = c(domain, total_n), 
        values_fill = 0
    ) |> 
    mutate(d = Diverse / Regular) |> 
    mutate(d = log(d)) |> 
    mutate(condition = case_when(
        d < -0.05 ~ "Regular",
        d > 0.05 ~ "Diverse",
        between(d, -.05, .05) ~ "Both"
    )) |>
    mutate(condition = factor(condition, c("Diverse", "Both", "Regular"))) |> 
    filter(total_n >= 100) |> 
    group_by(condition) |> 
    slice_max(n = 5, order_by = abs(d)) |> 
    ungroup() |> 
    arrange(condition, d) |> 
    gt(row_group_as_column = TRUE, groupname_col = "condition") |> 
    fmt_percent(3:4) |> 
    cols_label(d = "log(diverse/current)", domain = "Domain", total_n = "n")
```

### Diversity indicators

The diverse condition produces both more diverse individual sessions (see @tbl-diversity-by-sessions and @fig-diversity-indicators), as does it produce more diverse sources across all sessions over time (see @fig-diversity-over-time).

```{r}
#| echo: false
#| warning: false
#| label: tbl-diversity-by-sessions
#| tbl-cap: Differences between diversity indicators

links |> 
    group_by(uid) |> 
    count(domain) |> 
    mutate(p = (n/sum(n))*100) |> 
    arrange(-n) |> 
    summarise(
        hhi = hhi(n),
        sdi = sdi(n),
        unique_sources = sum(n)
    ) |> 
    pivot_longer(-uid) |> 
    mutate(condition = get_condition(uid)) |> 
    group_by(name) |> 
    summarise(
        t_test = broom::tidy(t.test(value ~ condition)),
        n = n()
    ) |> 
    unnest(t_test) |> 
    mutate(sig = ifelse(p.value < .001, "< .001", "not sig.")) |> 
    select(name, estimate1, estimate2, sig, n) |> 
    rename(Diverse = estimate2, Regular = estimate1) |> 
    gt() |> 
    fmt_number(Regular:Diverse) |> 
    cols_label(name = "Indicator", sig = "p.value") |> 
    tab_footnote("hhi: Herfindahl-Hirschman Index, sdi: Shannon Diversity Index, significance based on two-sided t-tests")
```

```{r}
#| echo: false
#| warning: false
#| label: fig-diversity-over-time
#| fig-cap: "Diversity Metrics over Time"
#| fig-alt: ""
#| fig-width: 8
#| fig-height: 4
links |> 
    group_by(date = floor_date(date, unit = "1 week"), condition) |> 
    count(domain, sort = TRUE) |> 
    summarise(
        hhi = hhi(n),
        sdi = sdi(n),
        unique_sources = n()
    ) |> 
    ungroup() |> 
    pivot_longer(hhi:unique_sources) |> 
    mutate(name = factor(name, levels = c("hhi", "sdi", "unique_sources"))) |> 
    ggplot(aes(x = as.Date(date), y = value, color = condition)) +
    geom_line(aes(group = date), color = "black") +
    geom_point(size = 2) +
    scale_x_date(date_labels = "KW %U", date_breaks = "2 weeks") +
    facet_wrap(vars(name), scales = "free") +
    labs(
        title = "Source diversity over time",
        subtitle = "Shannon Diversity Index (sdi): Higher values mean higher diversity,\nHerfindahl–Hirschman index (hhi): Higher values mean higher concentration",
        x = "Days",
        y = ""
    ) +
    theme(legend.position = "bottom")
```

```{r}
#| echo: false
#| warning: false
#| label: fig-diversity-indicators
#| fig-cap: "Distribution of Diversity Metrics by Condition"
#| fig-alt: ""
#| fig-width: 8
#| fig-height: 4
links |> 
    group_by(uid) |> 
    count(domain) |> 
    mutate(p = (n/sum(n))*100) |> 
    arrange(-n) |> 
    summarise(
        hhi = hhi(n),
        sdi = sdi(n),
        unique_sources = sum(n)
    ) |> 
    pivot_longer(-uid) |> 
    mutate(condition = get_condition(uid)) |> 
    ggplot(aes(x = value, fill = condition)) +
    geom_histogram(position = "dodge") +
    facet_wrap(vars(name), scales = "free") +
    theme(legend.position = "bottom")
```

### Website popularity

```{r}
test_df <- domains |> 
    group_by(domain) |> 
    mutate(total_n = sum(n)) |> 
    ungroup() |> 
    pivot_wider(
        names_from = condition, 
        values_from = p, 
        id_cols = c(domain, tranco_rank, total_n), 
        values_fill = 0
    ) |> 
    mutate(d = log(Diverse / Regular)) |> 
    mutate(condition = case_when(
        d < 0 ~ "Regular",
        d > 0 ~ "Diverse",
        TRUE ~ "Both"
    ))
```

In the diverse condition, ChatGPT linked to less popular[^3] websites than in the regular condition. Both in terms of number of references to sites listed in the tranco top 1 million (see @tbl-tranco-crosstab) and the average rank of linked sites (see @fig-tranco-condition). This effect is statistically significant but very small.

[^3]: i.e. sites being listed in the tranco top 1 million most visited sites

```{r}
#| echo: false
#| warning: false
#| label: tbl-tranco-crosstab
#| tbl-cap: Tranco Rank by Condition

fit <- table(
    !is.na(rep(test_df$tranco_rank, test_df$total_n)), 
    rep(test_df$condition, test_df$total_n)
) |> chisq.test()
test_df |> 
    group_by(condition) |> 
    select(tranco_rank, total_n, condition) |> 
    slice(rep(1:n(), total_n)) |> 
    ungroup() |> 
    count(condition, top_1m = !is.na(tranco_rank)) |> 
    group_by(condition) |> 
    mutate(p = n/sum(n)) |> 
    ungroup() |> 
    mutate(cell = sprintf("%i (%2.1f%%)", n, p*100)) |> 
    pivot_wider(names_from = condition, values_from = cell, id_cols = top_1m) |> 
    mutate(top_1m = ifelse(top_1m, "In top 1M", "Not in top 1M")) |> 
    gt() |> 
    fmt_tf(top_1m) |> 
    cols_label(top_1m = "Tranco Ranking") |> 
    tab_footnote(md("Chi<sup>2</sup> test = 162.78, p
< .001; Adjusted Cramer's v = 0.06"))
```

```{r}
#| echo: false
#| warning: false
#| label: fig-tranco-condition
#| fig-cap: "Differences in Tranco Ranks by Condition"
#| fig-alt: ""
#| fig-width: 8
#| fig-height: 4
fit <- t.test(rep(test_df$tranco_rank, test_df$total_n) ~ rep(test_df$condition, test_df$total_n))
test_df |> 
    group_by(condition) |> 
    select(tranco_rank, total_n) |> 
    slice(rep(1:n(), total_n)) |> 
    ungroup() |> 
    ggplot(aes(x = tranco_rank, fill = condition)) + 
    #geom_histogram(position = "dodge") +
    geom_density(alpha = .5) +
    labs(
        title = "Differences in Tranco Ranks by Condition",
        caption = "Welch Two Sample t-test: t(33076.99) = 14.37, p < .001; Cohen's d = 0.14",
        x = "Tranco Rank"
    )
```

##### Open Data {.appendix}

For code and data, see the [reproduction package](https://github.com/Kudusch/chatgpt_news/).
